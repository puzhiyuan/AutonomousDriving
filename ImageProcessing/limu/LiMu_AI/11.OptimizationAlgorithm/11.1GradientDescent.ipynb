{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 梯度下降"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 一维梯度下降"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import torch\n",
    "from d2l import torch as d2l\n",
    "\n",
    "def f(x):  # 目标函数\n",
    "    return x ** 2\n",
    "\n",
    "def f_grad(x):  # 目标函数的梯度(导数)\n",
    "    return 2 * x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将初始值设置为10,使用梯度下降迭代10次\n",
    "def gd(eta, f_grad):\n",
    "    x = 10.0\n",
    "    results = [x]\n",
    "    for i in range(10):\n",
    "        x -= eta * f_grad(x)\n",
    "        results.append(float(x))\n",
    "    print(f'epoch 10, x: {x:f}')\n",
    "    return results\n",
    "\n",
    "results = gd(0.2, f_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_trace(results, f):\n",
    "    n = max(abs(min(results)), abs(max(results)))\n",
    "    f_line = torch.arange(-n, n, 0.01)\n",
    "    d2l.set_figsize()\n",
    "    d2l.plot([f_line, results], [[f(x) for x in f_line], [lsf(x) for x in results]], 'x', 'f(x)', fmts=['-', '-o'])\n",
    "\n",
    "show_trace(results, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 学习率"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "show_trace(gd(0.05, f_grad), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_trace(gd(1.1, f_grad), f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 局部最小值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = torch.tensor(0.15 * np.pi)\n",
    "\n",
    "def f(x):  # 目标函数\n",
    "    return x * torch.cos(c * x)\n",
    "\n",
    "def f_grad(x):  # 目标函数的梯度\n",
    "    return torch.cos(c * x) - c * x * torch.sin(c * x)\n",
    "\n",
    "show_trace(gd(2, f_grad), f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 多元梯度下降"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_2d(trainer, steps=20, f_grad=None):\n",
    "    \"\"\"用定制的训练机优化2D目标函数\"\"\"\n",
    "    # s1和s2是稍后将使用的内部状态变量\n",
    "    x1, x2, s1, s2 = -5, -2, 0, 0\n",
    "    results = [(x1, x2)]\n",
    "    for i in range(steps):\n",
    "        if f_grad:\n",
    "            x1, x2, s1, s2 = trainer(x1, x2, s1, s2, f_grad)\n",
    "        else:\n",
    "            x1, x2, s1, s2 = trainer(x1, x2, s1, s2)\n",
    "        results.append((x1, x2))\n",
    "    print(f'epoch {i + 1}, x1: {float(x1):f}, x2: {float(x2):f}')\n",
    "    return results\n",
    "\n",
    "def show_trace_2d(f, results):\n",
    "    \"\"\"显示优化过程中2D变量的轨迹\"\"\"\n",
    "    d2l.set_figsize()\n",
    "    d2l.plt.plot(*zip(*results), '-o', color='#ff7f0e')\n",
    "    x1, x2 = torch.meshgrid(torch.arange(-5.5, 1.0, 0.1),\n",
    "                          torch.arange(-3.0, 1.0, 0.1), indexing='ij')\n",
    "    d2l.plt.contour(x1, x2, f(x1, x2), colors='#1f77b4')\n",
    "    d2l.plt.xlabel('x1')\n",
    "    d2l.plt.ylabel('x2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_2d(x1, x2):  # 目标函数\n",
    "    return x1 ** 2 + 2 * x2 ** 2\n",
    "\n",
    "def f_2d_grad(x1, x2):  # 目标函数的梯度\n",
    "    return (2 * x1, 4 * x2)\n",
    "\n",
    "def gd_2d(x1, x2, s1, s2, f_grad):\n",
    "    g1, g2 = f_grad(x1, x2)\n",
    "    return (x1 - eta * g1, x2 - eta * g2, 0, 0)\n",
    "\n",
    "eta = 0.1\n",
    "show_trace_2d(f_2d, train_2d(gd_2d, f_grad=f_2d_grad))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 自适应方法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 牛顿法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = torch.tensor(0.5)\n",
    "\n",
    "def f(x):  # O目标函数\n",
    "    return torch.cosh(c * x)\n",
    "\n",
    "def f_grad(x):  # 目标函数的梯度\n",
    "    return c * torch.sinh(c * x)\n",
    "\n",
    "def f_hess(x):  # 目标函数的Hessian\n",
    "    return c**2 * torch.cosh(c * x)\n",
    "\n",
    "def newton(eta=1):\n",
    "    x = 10.0\n",
    "    results = [x]\n",
    "    for i in range(10):\n",
    "        x -= eta * f_grad(x) / f_hess(x)\n",
    "        results.append(float(x))\n",
    "    print('epoch 10, x:', x)\n",
    "    return results\n",
    "\n",
    "show_trace(newton(), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = torch.tensor(0.15 * np.pi)\n",
    "\n",
    "def f(x):  # 目标函数\n",
    "    return x * torch.cos(c * x)\n",
    "\n",
    "def f_grad(x):  # 目标函数的梯度\n",
    "    return torch.cos(c * x) - c * x * torch.sin(c * x)\n",
    "\n",
    "def f_hess(x):  # 目标函数的Hessian\n",
    "    return - 2 * c * torch.sin(c * x) - x * c**2 * torch.cos(c * x)\n",
    "\n",
    "show_trace(newton(), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_trace(newton(0.5), f)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
