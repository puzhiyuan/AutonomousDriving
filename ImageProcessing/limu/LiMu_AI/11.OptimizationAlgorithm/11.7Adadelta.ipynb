{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adadelta\n",
    "Adadelta是AdaGrad的另一种变体， 主要区别在于前者减少了学习率适应坐标的数量。 此外，广义上Adadelta被称为没有学习率，因为它使用变化量本身作为未来变化的校准。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import torch\n",
    "from d2l import torch as d2l\n",
    "\n",
    "\n",
    "def init_adadelta_states(feature_dim):\n",
    "    s_w, s_b = torch.zeros((feature_dim, 1)), torch.zeros(1)\n",
    "    delta_w, delta_b = torch.zeros((feature_dim, 1)), torch.zeros(1)\n",
    "    return ((s_w, delta_w), (s_b, delta_b))\n",
    "\n",
    "def adadelta(params, states, hyperparams):\n",
    "    rho, eps = hyperparams['rho'], 1e-5\n",
    "    for p, (s, delta) in zip(params, states):\n",
    "        with torch.no_grad():\n",
    "            # In-placeupdatesvia[:]\n",
    "            s[:] = rho * s + (1 - rho) * torch.square(p.grad)\n",
    "            g = (torch.sqrt(delta + eps) / torch.sqrt(s + eps)) * p.grad\n",
    "            p[:] -= g\n",
    "            delta[:] = rho * delta + (1 - rho) * g * g\n",
    "        p.grad.data.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_iter, feature_dim = d2l.get_data_ch11(batch_size=10)\n",
    "d2l.train_ch11(adadelta, init_adadelta_states(feature_dim), {'rho': 0.9}, data_iter, feature_dim);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = torch.optim.Adadelta\n",
    "d2l.train_concise_ch11(trainer, {'rho': 0.9}, data_iter)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
