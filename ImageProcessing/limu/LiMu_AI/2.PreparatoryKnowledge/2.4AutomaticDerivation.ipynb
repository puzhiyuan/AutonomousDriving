{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 自动求导"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch中backward()方法详解\n",
    "**1. backward()方法介绍**\n",
    "- 在PyTorch中,对于一个计算图(Computational Graph),我们可以调用`.backward()`方法来进行反向传播,计算每个变量的梯度。\n",
    "- `.backward()`方法需要从计算图中指定的某个变量开始,按照链式法则自动计算并累积每个张量相对于其输入的梯度。计算图中所有叶子节点的`.grad`属性就存储了最终的梯度。\n",
    "\n",
    "**2. 使用backward()计算梯度**\n",
    "\n",
    "要使用`.backward()`计算梯度,主要分以下几步:\n",
    "\n",
    "- 构建包含可导变量(requires_grad=True)的计算图\n",
    "- 根据计算图进行前向传播,计算输出\n",
    "- 调用输出的`.backward()`方法启动反向传播\n",
    "- 计算图中可导变量的`.grad`属性包含了梯度\n",
    "\n",
    "例如:\n",
    "\n",
    "```python\n",
    "    x = torch.tensor(..., requires_grad=True)\n",
    "    y = 2 * x + 3\n",
    "    z = y**2\n",
    "    z.backward()\n",
    "    print(x.grad) # 4*x*2\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "**3. backward()方法的重要参数**\n",
    "- `retain_graph=True`:保留计算图进行多次反向传播\n",
    "- `create_graph=True`:进行高阶导数计算\n",
    "\n",
    "利用这些参数可以实现一些复杂的梯度计算技巧。\n",
    "\n",
    "**4. 总结**\n",
    "- .backward()自动计算梯度,是PyTorch的核心功能\n",
    "- 正确使用可以减少大量手动求导劳动\n",
    "- 需要理解计算图、链式法则等原理,才能灵活应用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-07T01:42:22.690509900Z",
     "start_time": "2023-08-07T01:42:22.680935100Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0., 1., 2., 3.])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "x = torch.arange(4.0)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-07T01:43:28.192088300Z",
     "start_time": "2023-08-07T01:43:28.186574900Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "# 反向传播中计算梯度,默认False\n",
    "x.requires_grad=True\n",
    "print(x.grad)#默认为None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-07T01:44:32.986677400Z",
     "start_time": "2023-08-07T01:44:32.978966700Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(28., grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "y = 2 * torch.dot(x, x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-07T01:45:41.576849900Z",
     "start_time": "2023-08-07T01:45:41.572772300Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.,  4.,  8., 12.])\n"
     ]
    }
   ],
   "source": [
    "y.backward()\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-07T01:48:56.463727800Z",
     "start_time": "2023-08-07T01:48:56.425404300Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([True, True, True, True])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad == 4 * x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-07T01:51:31.189985900Z",
     "start_time": "2023-08-07T01:51:31.185882800Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 1., 1., 1.])\n"
     ]
    }
   ],
   "source": [
    "# 梯度清零\n",
    "x.grad.zero_()\n",
    "y = x.sum()\n",
    "y.backward()\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-07T01:53:10.888916600Z",
     "start_time": "2023-08-07T01:53:10.868854300Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0., 2., 4., 6.])\n"
     ]
    }
   ],
   "source": [
    "x.grad.zero_()\n",
    "y = x * x\n",
    "y.sum().backward()\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-07T01:55:41.324015600Z",
     "start_time": "2023-08-07T01:55:41.304596400Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([True, True, True, True])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad.zero_()\n",
    "y = x * x\n",
    "u = y.detach()\n",
    "z = u * x\n",
    "\n",
    "z.sum().backward()\n",
    "x.grad == u"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch中detach()的作用\n",
    "detach()是PyTorch中非常重要的一个操作,它可以从计算图中分离出一个Tensor,使其不参与梯度反向传播。具体总结如下:\n",
    "\n",
    "- detach()会返回一个新的Tensor,它与原Tensor共享数据,但已经从计算图中分离\n",
    "- 新Tensor不再依赖计算图,所以在反向传播中,到它这就不会再递归计算梯度了\n",
    "- 因此可以通过detach()防止某些Tensor的梯度计算和更新\n",
    "- 如果只想断开某个中间变量的依赖,可以对其调用.detach()\n",
    "- detach()返回的Tensor还在同一个设备上,没有复制数据\n",
    "- 注意detach后就无法再计算这个Tensor的梯度了,因为已经从计算图分离\n",
    "- detach()不同于requires_grad=False,后者可以再打开求梯度,但detach()是永久分离\n",
    "- 正确使用detach可以提高效率,避免不必要的梯度计算\n",
    "\n",
    "所以detach()非常适合在以下场景中使用:\n",
    "- 冻结模型参数,防止更新\n",
    "- 断开不需要进行梯度回传的中间变量\n",
    "- 在纯前向推断时增加效率\n",
    "  \n",
    "***需要注意的是,只要需要继续求梯度,就不能detach,否则会导致梯度无法回传。***\n",
    "\n",
    "总之,detach()是计算图和自动求导中的一个非常重要的操作,合理利用可以让训练更高效。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-07T01:58:16.109395100Z",
     "start_time": "2023-08-07T01:58:16.103359900Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([True, True, True, True])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad.zero_()\n",
    "y.sum().backward()\n",
    "x.grad == 2 * x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-07T02:01:53.871473300Z",
     "start_time": "2023-08-07T02:01:53.866465400Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(True)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def f(a):\n",
    "    b = a * 2\n",
    "    # b的向量模长小于1000时\n",
    "    while b.norm() < 1000:\n",
    "        b = b * 2\n",
    "    if b.sum() > 0:\n",
    "        c = b\n",
    "    else:\n",
    "        c = 100 * b\n",
    "    return c\n",
    "\n",
    "a = torch.randn(size=(), requires_grad=True)\n",
    "d = f(a)\n",
    "d.backward()\n",
    "a.grad == d/a"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
